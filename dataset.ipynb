{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "s2KKHasKFvUL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                            postId  \\\n",
      "0           0  9ee74b02055311ebb757646e69d991ea   \n",
      "1           1  9ee74b02055311ebb757646e69d991ea   \n",
      "2           2  9ee74b02055311ebb757646e69d991ea   \n",
      "3           3  9ee74b02055311ebb757646e69d991ea   \n",
      "4           4  9ee74b02055311ebb757646e69d991ea   \n",
      "\n",
      "                                             comment  score      topic  \n",
      "0  بعيدا عن بعض التعليق العنصرية ،الامازيغية لغة ...    -36  tamazight  \n",
      "1  كان اكبر خطء ارتكبه المغرب نتيجة الخريف العربي...     66  tamazight  \n",
      "2  كا نتوقع من شي وحدين بلا حشما بلا حشوم غادي قو...    -47  tamazight  \n",
      "3  ارجوا من الاخوة المسؤولين احترام اللغة الأمازي...    -32  tamazight  \n",
      "4  تدريس الامازيغية مضيعة للوقت والمال وفتح باب ا...     55  tamazight  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# Load the data into a pandas dataframe\n",
    "df = pd.read_csv('comments_tamazight.csv')\n",
    "\n",
    "# Show the first few rows of the dataframe\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cvFcXwDrS84L"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       -36\n",
      "1        66\n",
      "2       -47\n",
      "3       -32\n",
      "4        55\n",
      "         ..\n",
      "59443     9\n",
      "59444     3\n",
      "59445     4\n",
      "59446     2\n",
      "59447     2\n",
      "Name: score, Length: 59448, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fe2Pc3EUwHzD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kafka.python in c:\\users\\huawei\\anaconda3\\lib\\site-packages (2.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install kafka.python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\huawei\\anaconda3\\lib\\site-packages (2.11.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (23.1.21)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.30.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.21.5)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (63.4.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.51.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (15.0.6.1)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.19.6)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.3.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.16.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.11.0->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\huawei\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1338/1338 [==============================] - 12s 8ms/step - loss: -2341433.5000 - accuracy: 0.0327 - val_loss: -15268865.0000 - val_accuracy: 0.0352\n",
      "Epoch 2/5\n",
      "1338/1338 [==============================] - 10s 8ms/step - loss: -115813384.0000 - accuracy: 0.0326 - val_loss: -336835296.0000 - val_accuracy: 0.0352\n",
      "Epoch 3/5\n",
      "1338/1338 [==============================] - 10s 7ms/step - loss: -885764672.0000 - accuracy: 0.0327 - val_loss: -1806608384.0000 - val_accuracy: 0.0352\n",
      "Epoch 4/5\n",
      "1338/1338 [==============================] - 8s 6ms/step - loss: -3205910272.0000 - accuracy: 0.0327 - val_loss: -5425373696.0000 - val_accuracy: 0.0352\n",
      "Epoch 5/5\n",
      "1338/1338 [==============================] - 10s 8ms/step - loss: -8337538560.0000 - accuracy: 0.0327 - val_loss: -12789376000.0000 - val_accuracy: 0.0352\n",
      "1858/1858 [==============================] - 4s 2ms/step\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "import csv \n",
    "import json \n",
    "\n",
    "def csv_to_json(csvFilePath, jsonFilePath):\n",
    "    jsonArray = []\n",
    "      \n",
    "    #read csv file\n",
    "    with open(csvFilePath, encoding='utf-8') as csvf: \n",
    "        #load csv file data using csv library's dictionary reader\n",
    "        csvReader = csv.DictReader(csvf) \n",
    "\n",
    "        #convert each csv row into python dict\n",
    "        for row in csvReader: \n",
    "            #add this python dict to json array\n",
    "            jsonArray.append(row)\n",
    "  \n",
    "    #convert python jsonArray to JSON String and write to file\n",
    "    with open(jsonFilePath, 'w', encoding='utf-8') as jsonf: \n",
    "        jsonString = json.dumps(jsonArray, indent=4)\n",
    "        jsonf.write(jsonString)\n",
    "          \n",
    "csvFilePath = r'comments_tamazight.csv'\n",
    "jsonFilePath = r'comments.json'\n",
    "\n",
    "csv_to_json(csvFilePath, jsonFilePath)\n",
    "\n",
    "# Load the existing dataset into a pandas dataframe\n",
    "df = pd.read_csv(\"comments_tamazight.csv\")\n",
    "\n",
    "# Randomly sample a portion of the data\n",
    "df = df.sample(frac=0.9, random_state=1)\n",
    "\n",
    "# Preprocess the text data\n",
    "comments = df['comment'].tolist()\n",
    "scores = df['score'].tolist()\n",
    "\n",
    "# Tokenize the comments\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(comments)\n",
    "sequence = tokenizer.texts_to_sequences(comments)\n",
    "padded = pad_sequences(sequence, maxlen=100)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(padded, scores, test_size=0.2, random_state=1)\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "\n",
    "# Create the deep neural network model\n",
    "model = Sequential([\n",
    "    Embedding(10000, 32),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_data=(X_val, y_val), batch_size=32)\n",
    "\n",
    "# Load the scraped data into a pandas dataframe\n",
    "scraped_data = pd.read_json(\"comments.json\")\n",
    "\n",
    "# Extract the text field from the scraped data\n",
    "scraped_comments = scraped_data['comment'].tolist()\n",
    "\n",
    "# Preprocess the scraped comments\n",
    "scraped_sequence = tokenizer.texts_to_sequences(scraped_comments)\n",
    "scraped_padded = pad_sequences(scraped_sequence, maxlen=100)\n",
    "\n",
    "# Make predictions on the scraped comments\n",
    "scraped_scores = model.predict(scraped_padded)\n",
    "print(scraped_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "9ono9NjvToFh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1338/1338 [==============================] - 9s 6ms/step - loss: -1303616.7500 - accuracy: 0.0328 - val_loss: -7808080.5000 - val_accuracy: 0.0352\n",
      "Epoch 2/5\n",
      "1338/1338 [==============================] - 8s 6ms/step - loss: -62066264.0000 - accuracy: 0.0327 - val_loss: -171239296.0000 - val_accuracy: 0.0352\n",
      "Epoch 3/5\n",
      "1338/1338 [==============================] - 8s 6ms/step - loss: -438225344.0000 - accuracy: 0.0327 - val_loss: -894357248.0000 - val_accuracy: 0.0352\n",
      "Epoch 4/5\n",
      "1338/1338 [==============================] - 7s 6ms/step - loss: -1642217216.0000 - accuracy: 0.0327 - val_loss: -2801021184.0000 - val_accuracy: 0.0352\n",
      "Epoch 5/5\n",
      "1338/1338 [==============================] - 8s 6ms/step - loss: -4276745216.0000 - accuracy: 0.0327 - val_loss: -6516296192.0000 - val_accuracy: 0.0352\n",
      "1858/1858 [==============================] - 2s 1ms/step\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "\n",
    "# Load the existing dataset into a pandas dataframe\n",
    "df = pd.read_csv(\"comments_tamazight.csv\")\n",
    "\n",
    "# Randomly sample a portion of the data\n",
    "df = df.sample(frac=0.9, random_state=1)\n",
    "\n",
    "# Preprocess the text data\n",
    "comments = df['comment'].tolist()\n",
    "scores = df['score'].tolist()\n",
    "\n",
    "# Tokenize the comments\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(comments)\n",
    "sequence = tokenizer.texts_to_sequences(comments)\n",
    "padded = pad_sequences(sequence, maxlen=100)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(padded, scores, test_size=0.2, random_state=1)\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "\n",
    "# Create the deep neural network model\n",
    "model = Sequential([\n",
    "    Embedding(10000, 32),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_data=(X_val, y_val), batch_size=32)\n",
    "\n",
    "# Load the scraped data into a pandas dataframe\n",
    "scraped_data = pd.read_json(\"comments.json\")\n",
    "\n",
    "# Extract the text field from the scraped data\n",
    "scraped_comments = scraped_data['comment'].tolist()\n",
    "\n",
    "# Preprocess the scraped comments\n",
    "scraped_sequence = tokenizer.texts_to_sequences(scraped_comments)\n",
    "scraped_padded = pad_sequences(scraped_sequence, maxlen=100)\n",
    "\n",
    "# Make predictions on the scraped comments\n",
    "scraped_scores = model.predict(scraped_padded)\n",
    "print(scraped_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "-kKLhTLPj6sN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 11 -37  23 ...   2   2  42]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load the existing dataset into a pandas dataframe\n",
    "df = pd.read_csv(\"comments_tamazight.csv\")\n",
    "\n",
    "# Select a smaller subset of the data\n",
    "subset = df.sample(frac=0.1)\n",
    "\n",
    "# Preprocess the text data\n",
    "vectorizer = CountVectorizer()\n",
    "features = vectorizer.fit_transform(subset['comment'])\n",
    "\n",
    "# Train the model\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(features, subset['score'])\n",
    "\n",
    "# Load the scraped data into a pandas dataframe\n",
    "scraped_data = pd.read_json(\"comments.json\")\n",
    "\n",
    "# Extract the text field from the scraped data\n",
    "scraped_comments = scraped_data['comment'].tolist()\n",
    "\n",
    "# Preprocess the scraped comments\n",
    "scraped_features = vectorizer.transform(scraped_comments)\n",
    "\n",
    "# Make predictions on the scraped comments\n",
    "scraped_scores = model.predict(scraped_features)\n",
    "print(scraped_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qJmtr7eukPeB"
   },
   "outputs": [],
   "source": [
    "# Evaluate the accuracy of the model\n",
    "accuracy = model.score(features, subset['score'])\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "emH6QrbhkSAh"
   },
   "outputs": [],
   "source": [
    "# Create a new dataframe from the scraped comments and their scores\n",
    "scraped_df = pd.DataFrame({'comments': scraped_comments, 'scores': scraped_scores})\n",
    "\n",
    "# Iterate over the rows of the dataframe and print the comment and its score\n",
    "for index, row in scraped_df.iterrows():\n",
    "    print(\"Comment: \", row['comments'])\n",
    "    print(\"Score: \", row['scores'])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b8zpiMPEkUSk"
   },
   "outputs": [],
   "source": [
    "print(scraped_df['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k-7gyb3ekYUs"
   },
   "outputs": [],
   "source": [
    "#Create a sentiment column\n",
    "sentiments = []\n",
    "for score in scraped_scores:\n",
    "    if score > 0:\n",
    "        sentiments.append(\"positive\")\n",
    "    elif score < 0:\n",
    "        sentiments.append(\"negative\")\n",
    "    else:\n",
    "        sentiments.append(\"neutral\")\n",
    "\n",
    "#Add the sentiment column to the scraped_data dataframe\n",
    "scraped_data['sentiment'] = sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oPwkqx0bkabx"
   },
   "outputs": [],
   "source": [
    "print(scraped_data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g27oop6zkc7j"
   },
   "outputs": [],
   "source": [
    "print(scraped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kRZrU30_ke44"
   },
   "outputs": [],
   "source": [
    "#Create a new column in the scraped_data dataframe with the sentiment labels\n",
    "scraped_data['sentiment'] = ['positive' if score > 0 else 'negative' if score < 0 else 'neutral' for score in scraped_scores]\n",
    "\n",
    "#Save the updated scraped_data dataframe to a json file\n",
    "scraped_data.to_json(\"scraped_data_with_sentiment.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X8jg5DU-kl3w"
   },
   "outputs": [],
   "source": [
    "print(scraped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQ1JJEklkl8t"
   },
   "outputs": [],
   "source": [
    "# Save the updated scraped_data to a json file\n",
    "scraped_data.to_json(\"scraped_data_with_sentiment.json\", orient='records', force_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
