{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPytQiFTKgiPkC3+uJPJZu8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asmy101/challenge-big-data/blob/main/dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlzTrP3P6bhB"
      },
      "outputs": [],
      "source": [
        "from google.colab import files \n",
        "from IPython.display import Image\n",
        "uploa=files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files \n",
        "from IPython.display import Image\n",
        "uploa=files.upload()"
      ],
      "metadata": {
        "id": "IuGu4-2SnFGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "# Load the data into a pandas dataframe\n",
        "df = pd.read_csv('comments_tamazight.csv')\n",
        "\n",
        "# Show the first few rows of the dataframe\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "s2KKHasKFvUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['score'])"
      ],
      "metadata": {
        "id": "cvFcXwDrS84L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install kafka.python"
      ],
      "metadata": {
        "id": "fe2Pc3EUwHzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
        "import csv \n",
        "import json \n",
        "\n",
        "def csv_to_json(csvFilePath, jsonFilePath):\n",
        "    jsonArray = []\n",
        "      \n",
        "    #read csv file\n",
        "    with open(csvFilePath, encoding='utf-8') as csvf: \n",
        "        #load csv file data using csv library's dictionary reader\n",
        "        csvReader = csv.DictReader(csvf) \n",
        "\n",
        "        #convert each csv row into python dict\n",
        "        for row in csvReader: \n",
        "            #add this python dict to json array\n",
        "            jsonArray.append(row)\n",
        "  \n",
        "    #convert python jsonArray to JSON String and write to file\n",
        "    with open(jsonFilePath, 'w', encoding='utf-8') as jsonf: \n",
        "        jsonString = json.dumps(jsonArray, indent=4)\n",
        "        jsonf.write(jsonString)\n",
        "          \n",
        "csvFilePath = r'comments_tamazight.csv'\n",
        "jsonFilePath = r'comments.json'\n",
        "\n",
        "csv_to_json(csvFilePath, jsonFilePath)\n",
        "\n",
        "# Load the existing dataset into a pandas dataframe\n",
        "df = pd.read_csv(\"comments_tamazight.csv\")\n",
        "\n",
        "# Randomly sample a portion of the data\n",
        "df = df.sample(frac=0.9, random_state=1)\n",
        "\n",
        "# Preprocess the text data\n",
        "comments = df['comment'].tolist()\n",
        "scores = df['score'].tolist()\n",
        "\n",
        "# Tokenize the comments\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(comments)\n",
        "sequence = tokenizer.texts_to_sequences(comments)\n",
        "padded = pad_sequences(sequence, maxlen=100)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(padded, scores, test_size=0.2, random_state=1)\n",
        "y_train = np.array(y_train)\n",
        "y_val = np.array(y_val)\n",
        "\n",
        "\n",
        "# Create the deep neural network model\n",
        "model = Sequential([\n",
        "    Embedding(10000, 32),\n",
        "    GlobalAveragePooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=5, validation_data=(X_val, y_val), batch_size=32)\n",
        "\n",
        "# Load the scraped data into a pandas dataframe\n",
        "scraped_data = pd.read_json(\"comments.json\")\n",
        "\n",
        "# Extract the text field from the scraped data\n",
        "scraped_comments = scraped_data['comment'].tolist()\n",
        "\n",
        "# Preprocess the scraped comments\n",
        "scraped_sequence = tokenizer.texts_to_sequences(scraped_comments)\n",
        "scraped_padded = pad_sequences(scraped_sequence, maxlen=100)\n",
        "\n",
        "# Make predictions on the scraped comments\n",
        "scraped_scores = model.predict(scraped_padded)\n",
        "print(scraped_scores)"
      ],
      "metadata": {
        "id": "9ono9NjvToFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load the existing dataset into a pandas dataframe\n",
        "df = pd.read_csv(\"comments_tamazight.csv\")\n",
        "\n",
        "# Select a smaller subset of the data\n",
        "subset = df.sample(frac=0.1)\n",
        "\n",
        "# Preprocess the text data\n",
        "vectorizer = CountVectorizer()\n",
        "features = vectorizer.fit_transform(subset['comment'])\n",
        "\n",
        "# Train the model\n",
        "model = DecisionTreeClassifier()\n",
        "model.fit(features, subset['score'])\n",
        "\n",
        "# Load the scraped data into a pandas dataframe\n",
        "scraped_data = pd.read_json(\"comments.json\")\n",
        "\n",
        "# Extract the text field from the scraped data\n",
        "scraped_comments = scraped_data['comment'].tolist()\n",
        "\n",
        "# Preprocess the scraped comments\n",
        "scraped_features = vectorizer.transform(scraped_comments)\n",
        "\n",
        "# Make predictions on the scraped comments\n",
        "scraped_scores = model.predict(scraped_features)\n",
        "print(scraped_scores)"
      ],
      "metadata": {
        "id": "-kKLhTLPj6sN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the accuracy of the model\n",
        "accuracy = model.score(features, subset['score'])\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "metadata": {
        "id": "qJmtr7eukPeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new dataframe from the scraped comments and their scores\n",
        "scraped_df = pd.DataFrame({'comments': scraped_comments, 'scores': scraped_scores})\n",
        "\n",
        "# Iterate over the rows of the dataframe and print the comment and its score\n",
        "for index, row in scraped_df.iterrows():\n",
        "    print(\"Comment: \", row['comments'])\n",
        "    print(\"Score: \", row['scores'])\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "emH6QrbhkSAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(scraped_df['scores'])"
      ],
      "metadata": {
        "id": "b8zpiMPEkUSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a sentiment column\n",
        "sentiments = []\n",
        "for score in scraped_scores:\n",
        "    if score > 0:\n",
        "        sentiments.append(\"positive\")\n",
        "    elif score < 0:\n",
        "        sentiments.append(\"negative\")\n",
        "    else:\n",
        "        sentiments.append(\"neutral\")\n",
        "\n",
        "#Add the sentiment column to the scraped_data dataframe\n",
        "scraped_data['sentiment'] = sentiments"
      ],
      "metadata": {
        "id": "k-7gyb3ekYUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(scraped_data['sentiment'])"
      ],
      "metadata": {
        "id": "oPwkqx0bkabx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(scraped_data)"
      ],
      "metadata": {
        "id": "g27oop6zkc7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a new column in the scraped_data dataframe with the sentiment labels\n",
        "scraped_data['sentiment'] = ['positive' if score > 0 else 'negative' if score < 0 else 'neutral' for score in scraped_scores]\n",
        "\n",
        "#Save the updated scraped_data dataframe to a json file\n",
        "scraped_data.to_json(\"scraped_data_with_sentiment.json\")"
      ],
      "metadata": {
        "id": "kRZrU30_ke44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(scraped_data)"
      ],
      "metadata": {
        "id": "X8jg5DU-kl3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the updated scraped_data to a json file\n",
        "scraped_data.to_json(\"scraped_data_with_sentiment.json\", orient='records', force_ascii=False)"
      ],
      "metadata": {
        "id": "WQ1JJEklkl8t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}